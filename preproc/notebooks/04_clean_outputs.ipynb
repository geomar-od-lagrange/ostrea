{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# Clean outputs\n",
    "\n",
    "Post-processing step after notebooks 02 and 03.\n",
    "\n",
    "Removes features with NaN corner coordinates from `hexes.geojson`\n",
    "and drops orphan rows referencing those hex IDs from all connectivity parquets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000002",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport math\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nOUT_DIR = Path(\"../../database/data\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000003",
   "metadata": {},
   "outputs": [],
   "source": "# Find hex IDs with NaN or Inf corner coordinates\nwith open(OUT_DIR / \"hexes.geojson\") as f:\n    gj = json.load(f)\n\nbad_ids = set()\nfor feat in gj[\"features\"]:\n    coords = feat[\"geometry\"][\"coordinates\"][0]\n    if any(not math.isfinite(v) for pt in coords for v in pt):\n        bad_ids.add(feat[\"properties\"][\"id\"])\n\nprint(f\"Hex IDs with NaN/Inf corners: {sorted(bad_ids)} ({len(bad_ids)} total)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000004",
   "metadata": {},
   "outputs": [],
   "source": "# Drop bad features from hexes.geojson and meta.json\nbefore = len(gj[\"features\"])\ngj[\"features\"] = [f for f in gj[\"features\"] if f[\"properties\"][\"id\"] not in bad_ids]\nwith open(OUT_DIR / \"hexes.geojson\", \"w\") as f:\n    json.dump(gj, f)\nprint(f\"hexes.geojson: {before} -> {len(gj['features'])} features\")\n\n# meta.json is columnar: {\"id\": {\"0\": 0, ...}, \"lon\": {...}, ...}\nwith open(OUT_DIR / \"meta.json\") as f:\n    meta = json.load(f)\nif bad_ids:\n    ids_col = meta[\"id\"]  # {\"0\": 0, \"1\": 1, ...}\n    bad_row_keys = {k for k, v in ids_col.items() if v in bad_ids}\n    before_meta = len(ids_col)\n    meta = {col: {k: v for k, v in col_data.items() if k not in bad_row_keys}\n            for col, col_data in meta.items()}\n    with open(OUT_DIR / \"meta.json\", \"w\") as f:\n        json.dump(meta, f)\n    print(f\"meta.json: {before_meta} -> {len(meta['id'])} entries\")\nelse:\n    print(f\"meta.json: no bad IDs, unchanged ({len(meta['id'])} entries)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000005",
   "metadata": {},
   "outputs": [],
   "source": "# Drop orphan rows from all connectivity parquets, round weights to 3 sig figs, and drop zero-weight rows\nfor path in sorted(OUT_DIR.glob(\"connectivity_*.pq\")):\n    df = pd.read_parquet(path)\n    mask = df[\"end_id\"].astype(int).isin(bad_ids) | df[\"start_id\"].isin(bad_ids)\n    dropped = mask.sum()\n    df = df[~mask].copy()\n    w = df[\"weight\"].values\n    exp = np.floor(np.log10(w))\n    df[\"weight\"] = np.round(w * 10 ** (2 - exp)) / 10 ** (2 - exp)\n    before_zero = len(df)\n    df = df[df[\"weight\"] > 0]\n    dropped_zero = before_zero - len(df)\n    df.to_parquet(path, index=False)\n    print(f\"{path.name}: dropped {dropped} orphans, {dropped_zero} zero-weight rows, {len(df):,} remaining\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}