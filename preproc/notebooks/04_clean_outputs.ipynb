{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1000001",
   "metadata": {},
   "source": [
    "# Clean outputs\n",
    "\n",
    "Post-processing step after notebooks 02 and 03.\n",
    "\n",
    "Removes features with NaN corner coordinates from `hexes.geojson`\n",
    "and drops orphan rows referencing those hex IDs from all connectivity parquets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000002",
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport math\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\n_cwd = Path.cwd()\nif (_cwd / \"../database/data\").exists():\n    OUT_DIR = (_cwd / \"../database/data\").resolve()\nelse:\n    OUT_DIR = (_cwd / \"../../database/data\").resolve()\nprint(f\"OUT_DIR: {OUT_DIR}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000003",
   "metadata": {},
   "outputs": [],
   "source": "# Find hex IDs with NaN or Inf corner coordinates\nwith open(OUT_DIR / \"hexes.geojson\") as f:\n    gj = json.load(f)\n\nbad_ids = set()\nfor feat in gj[\"features\"]:\n    coords = feat[\"geometry\"][\"coordinates\"][0]\n    if any(not math.isfinite(v) for pt in coords for v in pt):\n        bad_ids.add(feat[\"properties\"][\"id\"])\n\nprint(f\"Hex IDs with NaN/Inf corners: {sorted(bad_ids)} ({len(bad_ids)} total)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000004",
   "metadata": {},
   "outputs": [],
   "source": "# Drop bad features from hexes.geojson and meta.json\nbefore = len(gj[\"features\"])\ngj[\"features\"] = [f for f in gj[\"features\"] if f[\"properties\"][\"id\"] not in bad_ids]\nwith open(OUT_DIR / \"hexes.geojson\", \"w\") as f:\n    json.dump(gj, f)\nprint(f\"hexes.geojson: {before} -> {len(gj['features'])} features\")\n\n# meta.json is columnar: {\"id\": {\"0\": 0, ...}, \"lon\": {...}, ...}\nwith open(OUT_DIR / \"meta.json\") as f:\n    meta = json.load(f)\nif bad_ids:\n    ids_col = meta[\"id\"]  # {\"0\": 0, \"1\": 1, ...}\n    bad_row_keys = {k for k, v in ids_col.items() if v in bad_ids}\n    before_meta = len(ids_col)\n    meta = {col: {k: v for k, v in col_data.items() if k not in bad_row_keys}\n            for col, col_data in meta.items()}\n    with open(OUT_DIR / \"meta.json\", \"w\") as f:\n        json.dump(meta, f)\n    print(f\"meta.json: {before_meta} -> {len(meta['id'])} entries\")\nelse:\n    print(f\"meta.json: no bad IDs, unchanged ({len(meta['id'])} entries)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1000005",
   "metadata": {},
   "outputs": [],
   "source": "# Drop orphan rows from all connectivity parquets, round weights to 3 sig figs, and drop zero-weight rows\nfor path in sorted(OUT_DIR.glob(\"connectivity_*.pq\")):\n    df = pd.read_parquet(path)\n    mask = df[\"end_id\"].astype(int).isin(bad_ids) | df[\"start_id\"].isin(bad_ids)\n    dropped = mask.sum()\n    df = df[~mask].copy()\n    w = df[\"weight\"].values\n    exp = np.floor(np.log10(w))\n    df[\"weight\"] = np.round(w * 10 ** (2 - exp)) / 10 ** (2 - exp)\n    before_zero = len(df)\n    df = df[df[\"weight\"] > 0]\n    dropped_zero = before_zero - len(df)\n    df.to_parquet(path, index=False)\n    print(f\"{path.name}: dropped {dropped} orphans, {dropped_zero} zero-weight rows, {len(df):,} remaining\")"
  },
  {
   "cell_type": "markdown",
   "id": "kr05uztc5f",
   "source": "## Prune small disconnected hex clusters\n\nUses cube-coordinate adjacency to find connected components of the hex grid.\nComponents with â‰¤ 3 hexes are removed from all data files.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cetajcvh1fj",
   "source": "MIN_CLUSTER_SIZE = 4   # remove components with fewer than this many hexes\n\n# Parse cube coordinates from hex label strings like \"(-1, -19, 20)\"\nwith open(OUT_DIR / \"hex_label_to_id.json\") as f:\n    label_to_id = json.load(f)\n\ndef parse_cube(label: str):\n    return tuple(int(x) for x in label.strip(\"()\").split(\",\"))\n\ncubes = {parse_cube(lbl): int(hex_id) for lbl, hex_id in label_to_id.items()}\ncube_set = set(cubes)\n\n# Six cube-coordinate neighbor directions\nDIRECTIONS = [(1,-1,0),(-1,1,0),(1,0,-1),(-1,0,1),(0,1,-1),(0,-1,1)]\n\n# BFS connected components\nvisited = set()\ncomponents = []   # list of sets of int IDs\n\nfor cube in cube_set:\n    if cube in visited:\n        continue\n    component = set()\n    queue = [cube]\n    while queue:\n        c = queue.pop()\n        if c in visited:\n            continue\n        visited.add(c)\n        component.add(cubes[c])\n        for d in DIRECTIONS:\n            nb = (c[0]+d[0], c[1]+d[1], c[2]+d[2])\n            if nb in cube_set and nb not in visited:\n                queue.append(nb)\n    components.append(component)\n\ncomponents.sort(key=len)\nprint(f\"Total components: {len(components)}\")\nprint(f\"Size distribution (size: count):\")\nfrom collections import Counter\nsize_counts = Counter(len(c) for c in components)\nfor size in sorted(size_counts):\n    print(f\"  {size:4d} hex: {size_counts[size]:4d} component(s)\")\n\nsmall_ids = set().union(*(c for c in components if len(c) < MIN_CLUSTER_SIZE))\nprint(f\"\\nHex IDs to prune (cluster size < {MIN_CLUSTER_SIZE}): {len(small_ids)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "wy53wuz9dol",
   "source": "# Remove small-cluster hexes from hexes.geojson, meta.json, and parquets\nwith open(OUT_DIR / \"hexes.geojson\") as f:\n    gj = json.load(f)\nbefore = len(gj[\"features\"])\ngj[\"features\"] = [f for f in gj[\"features\"] if f[\"properties\"][\"id\"] not in small_ids]\nwith open(OUT_DIR / \"hexes.geojson\", \"w\") as f:\n    json.dump(gj, f)\nprint(f\"hexes.geojson: {before} -> {len(gj['features'])} features\")\n\nwith open(OUT_DIR / \"meta.json\") as f:\n    meta = json.load(f)\nids_col = meta[\"id\"]\nbad_row_keys = {k for k, v in ids_col.items() if v in small_ids}\nbefore_meta = len(ids_col)\nmeta = {col: {k: v for k, v in col_data.items() if k not in bad_row_keys}\n        for col, col_data in meta.items()}\nwith open(OUT_DIR / \"meta.json\", \"w\") as f:\n    json.dump(meta, f)\nprint(f\"meta.json: {before_meta} -> {len(meta['id'])} entries\")\n\nfor path in sorted(OUT_DIR.glob(\"connectivity_*.pq\")):\n    df = pd.read_parquet(path)\n    before_pq = len(df)\n    mask = df[\"end_id\"].astype(int).isin(small_ids) | df[\"start_id\"].isin(small_ids)\n    df = df[~mask]\n    df.to_parquet(path, index=False)\n    print(f\"{path.name}: {before_pq:,} -> {len(df):,} rows ({mask.sum()} dropped)\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}