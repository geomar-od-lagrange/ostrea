{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# Compute connectivity matrix (one file)\n",
    "\n",
    "Processes a single file (`05m_00-07days`) via OPeNDAP using stride-75 chunking.\n",
    "Computes normalized relative dilution factors:\n",
    "\n",
    "$$F = \\frac{\\text{obs\\_sum}}{N_{\\text{hex0\\_sum}} \\cdot DT_h \\cdot n_{\\text{months\\_years}}} \\cdot \\frac{wf_{\\text{hex0}}}{wf_{\\text{hex1}}}$$\n",
    "\n",
    "Produces `database/data/connectivity.pq`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = (\n",
    "    \"https://data.geomar.de/thredds/dodsC/\"\n",
    "    \"20.500.12085/11cc2d8f-4039-49d3-aaab-04ce0fb23190/submission\"\n",
    ")\n",
    "\n",
    "ESCAPE_HEX = b\"(0, 0, 0)\"\n",
    "OUT_DIR = Path(\"../../database/data\")\n",
    "\n",
    "# Single file for viability test\n",
    "DEPTH = \"05m\"\n",
    "TIME = \"00-07days\"\n",
    "DT_H = 168  # hours in window\n",
    "TIME_LABEL = \"00d-07d\"\n",
    "STRIDE = 75  # hex0 rows per OPeNDAP request (~100 MB/chunk)\n",
    "\n",
    "def url(depth, time):\n",
    "    name = f\"040_connectivity_analysis_{depth}_{time}.nc\"\n",
    "    return f\"{BASE}/040_connectivity_analysis_{depth}/{name}\"\n",
    "\n",
    "# Load hex label → int ID mapping built in notebook 02\n",
    "with open(OUT_DIR / \"hex_label_to_id.json\") as f:\n",
    "    label_to_id = json.load(f)  # keys are str like \"(-1, -19, 20)\"\n",
    "\n",
    "print(f\"Loaded {len(label_to_id)} hex IDs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(url(DEPTH, TIME), engine=\"netcdf4\")\n",
    "print(ds)\n",
    "\n",
    "n_hex0 = ds.sizes[\"hex0\"]\n",
    "n_hex1 = ds.sizes[\"hex1\"]\n",
    "n_months = ds.sizes[\"month\"]\n",
    "n_years = ds.sizes[\"year\"]\n",
    "n_months_years = n_months * n_years\n",
    "print(f\"\\nhex0={n_hex0}, hex1={n_hex1}, month={n_months}, year={n_years}\")\n",
    "print(f\"n_months_years={n_months_years}, STRIDE={STRIDE}, chunks={int(np.ceil(n_hex0/STRIDE))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load per-hex metadata needed for normalization\n",
    "hex0_labels = ds[\"hex0\"].values          # byte strings\n",
    "hex1_labels = ds[\"hex1\"].values\n",
    "wf_hex0 = ds[\"water_fraction_hex0\"].values   # shape (hex0,)\n",
    "wf_hex1 = ds[\"water_fraction_hex1\"].values   # shape (hex1,)\n",
    "\n",
    "# Find escape hex index in hex1\n",
    "escape_mask_hex1 = hex1_labels == ESCAPE_HEX\n",
    "print(f\"Escape hex in hex1: {escape_mask_hex1.sum()} occurrence(s)\")\n",
    "\n",
    "# Encode hex labels as str for ID lookup (bytes → str)\n",
    "hex0_str = np.array([b.decode() for b in hex0_labels])\n",
    "hex1_str = np.array([b.decode() for b in hex1_labels])\n",
    "\n",
    "# hex1 IDs for non-escape hexes\n",
    "valid_hex1_mask = ~escape_mask_hex1\n",
    "hex1_ids = np.array([label_to_id.get(s, -1) for s in hex1_str])\n",
    "print(f\"hex1 IDs: min={hex1_ids[valid_hex1_mask].min()}, max={hex1_ids[valid_hex1_mask].max()}, missing={( hex1_ids[valid_hex1_mask] == -1).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6a7b8",
   "metadata": {},
   "outputs": [],
   "source": "# Process in stride-75 hex0 chunks, accumulate sparse records\nimport time as time_mod\nfrom tqdm.auto import tqdm\n\nrecords = []  # list of DataFrames\nn_chunks = int(np.ceil(n_hex0 / STRIDE))\n\nfor chunk_idx in tqdm(range(n_chunks), desc=\"hex0 chunks\"):\n    i0 = chunk_idx * STRIDE\n    i1 = min(i0 + STRIDE, n_hex0)\n    t0 = time_mod.time()\n\n    # Load obs for this hex0 slice: shape (month, year, STRIDE, hex1)\n    obs_chunk = ds[\"obs\"].isel(hex0=slice(i0, i1)).values  # triggers OPeNDAP request\n\n    elapsed = time_mod.time() - t0\n\n    # Sum over month and year axes (0 and 1) → shape (hex0_slice, hex1)\n    obs_sum = np.nansum(obs_chunk, axis=(0, 1))  # (slice, hex1)\n\n    # N_hex0_sum: total particle-hours from each source (include escape hex)\n    N_hex0_sum = obs_sum.sum(axis=1)  # (slice,)\n\n    for local_i in range(i1 - i0):\n        global_i = i0 + local_i\n        if N_hex0_sum[local_i] == 0:\n            continue\n        src_label = hex0_str[global_i]\n        src_id = label_to_id.get(src_label, -1)\n        if src_id == -1:\n            continue\n        wf0 = wf_hex0[global_i]\n        if wf0 == 0 or np.isnan(wf0):\n            continue\n\n        row = obs_sum[local_i]           # shape (hex1,)\n        # Only non-escape, non-zero targets\n        target_mask = valid_hex1_mask & (row > 0)\n        if target_mask.sum() == 0:\n            continue\n\n        tgt_ids = hex1_ids[target_mask]\n        tgt_obs = row[target_mask]\n        tgt_wf1 = wf_hex1[target_mask]\n\n        F = (tgt_obs / (N_hex0_sum[local_i] * DT_H * n_months_years)) * (wf0 / tgt_wf1)\n\n        df = pd.DataFrame({\n            \"start_id\": np.int64(src_id),\n            \"end_id\": tgt_ids.astype(str),\n            \"weight\": F,\n        })\n        records.append(df)\n\nds.close()\nprint(f\"\\nDone. {len(records)} source hex blocks.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a7b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate and tag\n",
    "conn = pd.concat(records, ignore_index=True)\n",
    "conn[\"depth\"] = DEPTH\n",
    "conn[\"time\"] = TIME_LABEL\n",
    "\n",
    "# Reorder columns to match schema\n",
    "conn = conn[[\"start_id\", \"end_id\", \"time\", \"depth\", \"weight\"]]\n",
    "\n",
    "print(conn.dtypes)\n",
    "print(f\"\\nRows: {len(conn):,}\")\n",
    "print(f\"weight range: {conn.weight.min():.3e} – {conn.weight.max():.3e}\")\n",
    "print(conn.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0",
   "metadata": {},
   "outputs": [],
   "source": "out_path = OUT_DIR / f\"connectivity_{DEPTH}_{TIME_LABEL}.pq\"\nconn.to_parquet(out_path, index=False)\nprint(f\"Written: {out_path} ({out_path.stat().st_size / 1e6:.1f} MB, {len(conn):,} rows)\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}